import numpy as np
import pyqtgraph as pg
import time


def measure_time_decorator(func):
    """
    measures the time of an function execution.
    """

    def wrapper(self, *args, **kwargs):
        start_time = time.perf_counter()
        value = func(self, *args, **kwargs)
        print(func, "called. elapsed time:", time.perf_counter() - start_time)
        return value

    return wrapper

class RLPerformanceMonitorBaseline:
    def __init__(self, rlAgent, graphicsWindow, visualOutput, reward_plot_viewbox=(-100, 10, 50),
                 steps_plot_viewbox=(0, 1000, 50)):

        # store the rlAgent
        self.rlAgent = rlAgent
        self.graphicsWindow = graphicsWindow

        # shall visual output be provided?
        self.visualOutput = visualOutput

        if visualOutput:
            self.layout = graphicsWindow.centralWidget
            # redefine the gui's dimensions
            self.graphicsWindow.setGeometry(0, 0, 1800, 600)

            # add labels
            self.sps_label = self.layout.addLabel(col=2, justify='center')
            self.sps_label.setFixedHeight(h=10)
            self.nb_episodes_label = self.layout.addLabel("Episode: 0", col=3, justify='center')
            self.nb_episodes_label.setFixedHeight(h=10)
            self.total_steps_label = self.layout.addLabel(col=4, justify='center')
            self.total_steps_label.setFixedHeight(h=10)

            # pens
            self.raw_pen = pg.mkPen((255, 255, 255), width=1)
            self.mean_pen = pg.mkPen((255, 0, 0), width=2)
            self.var_pen = pg.mkPen((0, 255, 0), width=2)

            # viewbox
            self.reward_plot_viewbox = pg.ViewBox(parent=self.layout, enableMouse=True, enableMenu=False)
            self.reward_plot_viewbox.setYRange(min=reward_plot_viewbox[0], max=reward_plot_viewbox[1])
            self.reward_plot_viewbox.setXRange(min=0, max=reward_plot_viewbox[2])
            self.reward_plot_viewbox.setAutoPan(x=True, y=False)
            self.reward_plot_viewbox.enableAutoRange(x=True, y=False)
            self.reward_plot_viewbox.setLimits(xMin=0)

            self.steps_plot_viewbox = pg.ViewBox(parent=self.layout, enableMouse=True, enableMenu=False)
            self.steps_plot_viewbox.setYRange(min=steps_plot_viewbox[0], max=steps_plot_viewbox[1])
            self.steps_plot_viewbox.setXRange(min=0, max=steps_plot_viewbox[2])
            self.steps_plot_viewbox.setAutoPan(x=True, y=False)
            self.steps_plot_viewbox.enableAutoRange(x=True, y=False)
            self.steps_plot_viewbox.setLimits(xMin=0)

            # episode plots
            self.reward_plot_item = self.layout.addPlot(title="reward", viewBox=self.reward_plot_viewbox,
                                                        colspan=3, col=2, row=1)
            self.reward_plot_item.showGrid(x=True, y=True)
            self.reward_graph = self.reward_plot_item.plot()
            self.mean_reward_graph = self.reward_plot_item.plot()

            self.steps_plot_item = self.layout.addPlot(title="steps per episode", viewBox=self.steps_plot_viewbox,
                                                       colspan=3, col=2, row=2)
            self.steps_plot_item.showGrid(x=True, y=True)
            self.steps_graph = self.steps_plot_item.plot()
            self.mean_steps_graph = self.steps_plot_item.plot()

            self.layout.nextRow()

            # the episode range for calculating means and variances
            self.calculation_range = 20

            # data traces
            self.reward_trace = []
            self.nb_episode_steps_trace = []
            self.mean_rewards_trace = []
            self.mean_nb_episode_steps_trace = []
            self.reward_variance_trace = []
            self.nb_episode_steps_variance_trace = []

            # save start time for sps calculation
            self.start_time = time.perf_counter()
            self.sps = 0


    '''
    This function clears the plots generated by the performance monitor.
    '''

    def clearPlots(self):
        if self.visualOutput:
            self.graphicsWindow.removeItem(self.rlRewardPlot)


    def set_episode_data(self, nb_episode, nb_episode_steps, cumulative_reward, nb_step):
        """
        update the episode data plots
        this is rather slow and is just done on the end of an episode
        :param nb_episode: number of the current episode
        :param nb_episode_steps: the number of steps of this episode
        :param cumulative_reward: the reward for this episode
        :return:
        """
        # calculate the average steps per second for each episode
        self.sps = int(nb_episode_steps / (time.perf_counter() - self.start_time))
        # reset start time
        self.start_time = time.perf_counter()
        # set the sps label
        self.__set_sps(self.sps)
        # display the total elapsed steps
        self.__set_nb_steps(nb_step)

        self.nb_episodes_label.setText(f'Episode: {nb_episode}')
        self.__set_episode_plot(nb_episode, nb_episode_steps, cumulative_reward)

    def __set_sps(self, sps):
        """
        set the sps value to the label
        :param sps: steps per second
        :return:
        """
        self.sps_label.setText("steps per second: " + str(sps))

    def __set_nb_steps(self, steps):
        """
        set the number of steps to the label
        :param steps: total number of steps
        :return:
        """
        self.total_steps_label.setText("elapsed steps: " + str(steps))

    def __set_episode_plot(self, nb_episode, nb_episode_steps, episode_reward):
        """
        calculates the mean and the variance and plots the values in the corresponding graphs.
        :param nb_episode: number of the current episode
        :param nb_episode_steps: the number of steps of this episode
        :param episode_reward: the reward for this episode.
        :return:
        """
        # append data
        self.reward_trace.append(episode_reward)
        self.nb_episode_steps_trace.append(nb_episode_steps)

        # get the slices for mean calculation
        # if the mean calculation range exceeds the number of gathered value
        # use all existing data as slices
        # else get slices of mean calculation size.
        if nb_episode < self.calculation_range:
            reward_slice = self.reward_trace
            steps_slice = self.nb_episode_steps_trace
        else:
            reward_slice = self.reward_trace[-self.calculation_range:]
            steps_slice = self.nb_episode_steps_trace[-self.calculation_range:]

        # calculate the means
        mean_reward = np.mean(reward_slice)
        mean_steps = np.mean(steps_slice)

        # calculate variances
        var_reward = np.var(reward_slice)
        var_steps = np.var(steps_slice)

        # append the means
        self.mean_rewards_trace.append(mean_reward)
        self.mean_nb_episode_steps_trace.append(mean_steps)

        # append variances
        self.reward_variance_trace.append(var_reward)
        self.nb_episode_steps_variance_trace.append(var_steps)

        # plot series
        self.reward_graph.setData(self.reward_trace, pen=self.raw_pen)
        self.mean_reward_graph.setData(self.mean_rewards_trace, pen=self.mean_pen)

        self.steps_graph.setData(self.nb_episode_steps_trace, pen=self.raw_pen)
        self.mean_steps_graph.setData(self.mean_nb_episode_steps_trace, pen=self.mean_pen)


    '''
    This function is called when a trial ends. Here, information about the monitored variables is memorized, and the monitor graphs are updated.

    trial:  the actual trial number
    logs:   information from the reinforcement learning subsystem
    '''

    def update(self, trial, logs):
        # update the reward traces
        cumulative_reward = logs['episode_reward']
        nb_episode_steps = logs['nb_episode_steps']
        elased_steps = logs['nb_steps']

        # update the plots; trial number start from 0
        self.set_episode_data(trial+1, nb_episode_steps, cumulative_reward, elased_steps)

        pg.mkQApp().processEvents()
