# basic imports
import numpy as np
import time
import pyqtgraph as pg
                    

class RLPerformanceMonitorBaseline():
    '''
    Performance monitor. Used for tracking learning progress.
    
    | **Args**
    | rlAgent:                      Reference to the RL agent used.
    | trial:                        Maximum number of trials for which the experiment is run.
    | guiParent:                    The main window for visualization.
    | visualOutput:                 If true, the learning progress will be plotted.
    | rewardRange:                  The range for which the cumulative reward will be plotted.
    '''
    
    def __init__(self, rlAgent, trials, guiParent, visualOutput, rewardRange=[0, 1]):
        # store the rlAgent
        self.rlAgent = rlAgent
        self.guiParent = guiParent
        # shall visual output be provided?
        self.visualOutput = visualOutput
        #define the variables that will be monitored
        self.rlRewardTraceRaw = np.zeros(trials, dtype='float')
        self.rlRewardTraceRefined = np.zeros(trials, dtype='float')
        # this is the accumulation range for smoothing the reward curve
        self.accumulationRangeReward = 20
        # this is the accumulation interval for correct/incorrect decisions at the beginning/end of the single experimental phases (acquisition,extinction,renewal) 
        self.accumulationIntervalPerformance = 10
        
        if visualOutput:
            # redefine the gui's dimensions
            self.guiParent.setGeometry(50, 50, 1600, 600)
            # set up the required plots
            self.rlRewardPlot = self.guiParent.addPlot(title="Reinforcement learning progress")
            # set x/y-ranges for the plots
            self.rlRewardPlot.setXRange(0, trials)
            self.rlRewardPlot.setYRange(rewardRange[0], rewardRange[1])
            # define the episodes domain
            self.episodesDomain = np.linspace(0, trials, trials)
            # each variable has a dedicated graph that can be used for displaying the monitored values
            self.rlRewardTraceRawGraph=self.rlRewardPlot.plot(self.episodesDomain, self.rlRewardTraceRaw)
            self.rlRewardTraceRefinedGraph=self.rlRewardPlot.plot(self.episodesDomain, self.rlRewardTraceRefined)

    def clearPlots(self):
        '''
        This function clears the plots generated by the performance monitor.
        '''
        if self.visualOutput:
            self.guiParent.removeItem(self.rlRewardPlot)
    
    def update(self, trial, logs):
        '''
        This function is called when a trial ends. Here, information about the monitored variables is memorized, and the monitor graphs are updated.
        
        | **Args**
        | trial:                        The actual trial number.
        | logs:                         Information from the reinforcement learning subsystem.
        '''
        # update the reward traces
        rlReward = logs['episode_reward']
        self.rlRewardTraceRaw[trial] = rlReward
        # prepare aggregated reward trace
        aggregatedRewardTraceRaw = None
        if trial < self.accumulationRangeReward:
            aggregatedRewardTraceRaw = self.rlRewardTraceRaw[trial:None:-1]
        else:
            aggregatedRewardTraceRaw = self.rlRewardTraceRaw[trial:trial-self.accumulationRangeReward:-1]
        self.rlRewardTraceRefined[trial] = np.mean(aggregatedRewardTraceRaw)
        
        if self.visualOutput:
            # set the graph's data
            self.rlRewardTraceRawGraph.setData(self.episodesDomain, self.rlRewardTraceRaw, pen=pg.mkPen(color=(128, 128, 128), width=1))
            self.rlRewardTraceRefinedGraph.setData(self.episodesDomain, self.rlRewardTraceRefined, pen=pg.mkPen(color=(255, 0, 0), width=2))
            
            
class CRCMonitor():
    '''
    Performance monitor. used to record and display the CRC.
    
    | **Args**
    | rlAgent:                      Reference to the RL agent used.
    | trial:                        Maximum number of trials for which the experiment is run.
    | guiParent:                    The main window for visualization.
    | visualOutput:                 If true, the learning progress will be plotted.
    '''
    
    def __init__(self, rlAgent, trials, guiParent, visualOutput):
        # store the rlAgent
        self.rlAgent = rlAgent
        self.guiParent = guiParent
        # shall visual output be provided?
        self.visualOutput = visualOutput
        #define the variables that will be monitored
        self.responses = np.zeros(trials, dtype='float')
        self.CRC = np.zeros(trials, dtype='float')
        
        if visualOutput:
            # redefine the gui's dimensions
            self.guiParent.setGeometry(50, 50, 1600, 600)
            # set up the required plots
            self.CRCPlot = self.guiParent.addPlot(title="CRC")
            # set initial x/y-ranges of the plot
            self.CRCPlot.setXRange(0, 0)
            self.CRCPlot.setYRange(0, 0)
            # define the episodes domain
            self.episodesDomain = np.linspace(0, trials, trials)
            # make graph for CRC
            self.CRCGraph = self.CRCPlot.plot(self.episodesDomain, self.CRC)

    def clearPlots(self):
        '''
        This function clears the plots generated by the performance monitor.
        '''
        if self.visualOutput:
            self.guiParent.removeItem(self.rlRewardPlot)
    
    def update(self, trial, logs):
        '''
        This function is called when a trial ends. Here, information about the monitored variables is memorized, and the monitor graphs are updated.
        
        | **Args**
        | trial:                        The actual trial number.
        | logs:                         Information from the reinforcement learning subsystem.
        '''
        # store agent's response
        self.responses[trial] = logs['response']
        # update CRC
        self.CRC[trial] = np.sum(self.responses[:(trial + 1)])
        
        if self.visualOutput:
            # update x/y-ranges of the plot
            self.CRCPlot.setXRange(0, trial)
            self.CRCPlot.setYRange(np.amin(self.CRC), np.amax(self.CRC))
            # set the graph's data
            self.CRCGraph.setData(self.episodesDomain, self.CRC, pen=pg.mkPen(color=(128, 128, 128), width=1))
            
            
def measure_time_decorator(func):
    '''
    measures the time of an function execution.
    '''
    def wrapper(self, *args, **kwargs):
        start_time = time.perf_counter()
        value = func(self, *args, **kwargs)
        print(func, "called. elapsed time:", time.perf_counter() - start_time)
        return value

    return wrapper

class UnityPerformanceMonitor:
    def __init__(self, rlAgent, graphicsWindow, visualOutput, reward_plot_viewbox=(-100, 10, 50),
                 steps_plot_viewbox=(0, 1000, 50)):

        # store the rlAgent
        self.rlAgent = rlAgent
        self.graphicsWindow = graphicsWindow

        # shall visual output be provided?
        self.visualOutput = visualOutput

        if visualOutput:
            self.layout = graphicsWindow.centralWidget
            # redefine the gui's dimensions
            self.graphicsWindow.setGeometry(50, 50, 1600, 600)

            # add labels
            self.sps_label = self.layout.addLabel(col=2, justify='center')
            self.sps_label.setFixedHeight(h=10)
            self.nb_episodes_label = self.layout.addLabel("Episode: 0", col=3, justify='center')
            self.nb_episodes_label.setFixedHeight(h=10)
            self.total_steps_label = self.layout.addLabel(col=4, justify='center')
            self.total_steps_label.setFixedHeight(h=10)

            # pens
            self.raw_pen = pg.mkPen((255, 255, 255), width=1)
            self.mean_pen = pg.mkPen((255, 0, 0), width=2)
            self.var_pen = pg.mkPen((0, 255, 0), width=2)

            # viewbox
            self.reward_plot_viewbox = pg.ViewBox(parent=self.layout, enableMouse=True, enableMenu=False)
            self.reward_plot_viewbox.setYRange(min=reward_plot_viewbox[0], max=reward_plot_viewbox[1])
            self.reward_plot_viewbox.setXRange(min=0, max=reward_plot_viewbox[2])
            self.reward_plot_viewbox.setAutoPan(x=True, y=False)
            self.reward_plot_viewbox.enableAutoRange(x=True, y=False)
            self.reward_plot_viewbox.setLimits(xMin=0)

            self.steps_plot_viewbox = pg.ViewBox(parent=self.layout, enableMouse=True, enableMenu=False)
            self.steps_plot_viewbox.setYRange(min=steps_plot_viewbox[0], max=steps_plot_viewbox[1])
            self.steps_plot_viewbox.setXRange(min=0, max=steps_plot_viewbox[2])
            self.steps_plot_viewbox.setAutoPan(x=True, y=False)
            self.steps_plot_viewbox.enableAutoRange(x=True, y=False)
            self.steps_plot_viewbox.setLimits(xMin=0)

            # episode plots
            self.reward_plot_item = self.layout.addPlot(title="reward", viewBox=self.reward_plot_viewbox,
                                                        colspan=3, col=2, row=1)
            self.reward_plot_item.showGrid(x=True, y=True)
            self.reward_graph = self.reward_plot_item.plot()
            self.mean_reward_graph = self.reward_plot_item.plot()

            self.steps_plot_item = self.layout.addPlot(title="steps per episode", viewBox=self.steps_plot_viewbox,
                                                       colspan=3, col=2, row=2)
            self.steps_plot_item.showGrid(x=True, y=True)
            self.steps_graph = self.steps_plot_item.plot()
            self.mean_steps_graph = self.steps_plot_item.plot()

            self.layout.nextRow()

            # the episode range for calculating means and variances
            self.calculation_range = 20

            # data traces
            self.reward_trace = []
            self.nb_episode_steps_trace = []
            self.mean_rewards_trace = []
            self.mean_nb_episode_steps_trace = []
            self.reward_variance_trace = []
            self.nb_episode_steps_variance_trace = []

            # save start time for sps calculation
            self.start_time = time.perf_counter()
            self.sps = 0

    def clearPlots(self):
        '''
        This function clears the plots generated by the performance monitor.
        '''
        if self.visualOutput:
            self.graphicsWindow.removeItem(self.rlRewardPlot)


    def set_episode_data(self, nb_episode, nb_episode_steps, cumulative_reward, nb_step):
        '''
        update the episode data plots
        this is rather slow and is just done on the end of an episode
        :param nb_episode: number of the current episode
        :param nb_episode_steps: the number of steps of this episode
        :param cumulative_reward: the reward for this episode
        :return:
        '''
        # calculate the average steps per second for each episode
        self.sps = int(nb_episode_steps / (time.perf_counter() - self.start_time))
        # reset start time
        self.start_time = time.perf_counter()
        # set the sps label
        self.__set_sps(self.sps)
        # display the total elapsed steps
        self.__set_nb_steps(nb_step)
        self.nb_episodes_label.setText(f'Episode: {nb_episode}')
        self.__set_episode_plot(nb_episode, nb_episode_steps, cumulative_reward)

    def __set_sps(self, sps):
        '''
        set the sps value to the label
        :param sps: steps per second
        :return:
        '''
        self.sps_label.setText("steps per second: " + str(sps))

    def __set_nb_steps(self, steps):
        '''
        set the number of steps to the label
        :param steps: total number of steps
        :return:
        '''
        self.total_steps_label.setText("elapsed steps: " + str(steps))

    def __set_episode_plot(self, nb_episode, nb_episode_steps, episode_reward):
        '''
        calculates the mean and the variance and plots the values in the corresponding graphs.
        :param nb_episode: number of the current episode
        :param nb_episode_steps: the number of steps of this episode
        :param episode_reward: the reward for this episode.
        :return:
        '''
        # append data
        self.reward_trace.append(episode_reward)
        self.nb_episode_steps_trace.append(nb_episode_steps)

        # get the slices for mean calculation
        # if the mean calculation range exceeds the number of gathered value
        # use all existing data as slices
        # else get slices of mean calculation size.
        if nb_episode < self.calculation_range:
            reward_slice = self.reward_trace
            steps_slice = self.nb_episode_steps_trace
        else:
            reward_slice = self.reward_trace[-self.calculation_range:]
            steps_slice = self.nb_episode_steps_trace[-self.calculation_range:]

        # calculate the means
        mean_reward = np.mean(reward_slice)
        mean_steps = np.mean(steps_slice)

        # calculate variances
        var_reward = np.var(reward_slice)
        var_steps = np.var(steps_slice)

        # append the means
        self.mean_rewards_trace.append(mean_reward)
        self.mean_nb_episode_steps_trace.append(mean_steps)

        # append variances
        self.reward_variance_trace.append(var_reward)
        self.nb_episode_steps_variance_trace.append(var_steps)

        # plot series
        self.reward_graph.setData(self.reward_trace, pen=self.raw_pen)
        self.mean_reward_graph.setData(self.mean_rewards_trace, pen=self.mean_pen)

        self.steps_graph.setData(self.nb_episode_steps_trace, pen=self.raw_pen)
        self.mean_steps_graph.setData(self.mean_nb_episode_steps_trace, pen=self.mean_pen)

    def update(self, trial, logs):
        '''
        This function is called when a trial ends. Here, information about the monitored variables is memorized, and the monitor graphs are updated.
    
        trial:  the actual trial number
        logs:   information from the reinforcement learning subsystem
        '''
        # update the reward traces
        cumulative_reward = logs['episode_reward']
        nb_episode_steps = logs['nb_episode_steps']
        elased_steps = logs['nb_steps']

        # update the plots; trial number start from 0
        self.set_episode_data(trial+1, nb_episode_steps, cumulative_reward, elased_steps)

        pg.mkQApp().processEvents()